{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZkmaf6W+q+uUTvEVK1O/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartijnRoozendaal1/vvr-prediction/blob/main/video_data_preprocessing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Initialize Parameters and Face Alignment Network (FAN)"
      ],
      "metadata": {
        "id": "zSiojjTb-G0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "!pip install face-alignment\n",
        "import cv2\n",
        "import face_alignment\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Path to the folder containing infrared video files\n",
        "video_folder = '/content/drive/MyDrive/InfraredVideos'\n",
        "\n",
        "# Initialize the Face Alignment Network (FAN) for facial landmark detection\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device=device)\n"
      ],
      "metadata": {
        "id": "gRs8gfnu-Htc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Helper Functions"
      ],
      "metadata": {
        "id": "1GfRiIlJ-J6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_roi(frame, landmarks):\n",
        "    \"\"\"\n",
        "    Extracts regions of interest (ROIs) from a frame using facial landmarks.\n",
        "    ROIs include the eyes, nose, and cheeks.\n",
        "    \"\"\"\n",
        "    landmarks = landmarks.astype(int)  # Convert landmarks to integers\n",
        "    rois = {\n",
        "        'eyes': frame[min(landmarks[36:42, 1]):max(landmarks[36:42, 1]),\n",
        "                      min(landmarks[36:42, 0]):max(landmarks[36:42, 0])],\n",
        "        'nose': frame[min(landmarks[27:36, 1]):max(landmarks[27:36, 1]),\n",
        "                      min(landmarks[27:36, 0]):max(landmarks[27:36, 0])],\n",
        "        'cheeks': frame[min(landmarks[48:54, 1]):max(landmarks[48:54, 1]),\n",
        "                        min(landmarks[48:54, 0]):max(landmarks[48:54, 0])]\n",
        "    }\n",
        "    return rois\n",
        "\n",
        "def resample_frames(video_frames, target_count=25):\n",
        "    \"\"\"\n",
        "    Resamples frames to ensure a uniform count across videos.\n",
        "    If the number of frames is insufficient, repeats existing frames.\n",
        "    \"\"\"\n",
        "    if len(video_frames) >= target_count:\n",
        "        return video_frames[:target_count]  # Truncate to target_count\n",
        "    repeat_count = target_count - len(video_frames)\n",
        "    return video_frames + video_frames[:repeat_count]\n"
      ],
      "metadata": {
        "id": "Go6HxBKt-Oha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Process Videos and Extract ROIs"
      ],
      "metadata": {
        "id": "XZ7JS26d-WVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store processed frames\n",
        "processed_frames = {}\n",
        "\n",
        "# List to track excluded videos\n",
        "excluded_videos = []\n",
        "\n",
        "# Statistics dictionary to track processing details\n",
        "video_stats = {\"total_videos\": 0, \"total_frames\": 0, \"frames_per_video\": []}\n",
        "\n",
        "# Iterate through each video in the folder\n",
        "for video_name in tqdm(os.listdir(video_folder)):\n",
        "    if not video_name.endswith(('.mp4', '.wmv')):\n",
        "        continue  # Skip non-video files\n",
        "\n",
        "    video_stats[\"total_videos\"] += 1\n",
        "    video_path = os.path.join(video_folder, video_name)\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Select 25 evenly spaced frames\n",
        "    selected_frames_indices = np.linspace(0, frame_count - 1, 25).astype(int)\n",
        "    video_frames = []\n",
        "\n",
        "    for idx in range(frame_count):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or idx not in selected_frames_indices:\n",
        "            continue\n",
        "\n",
        "        # Convert frame to RGB for FAN\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        detected_landmarks = fa.get_landmarks(rgb_frame)\n",
        "\n",
        "        if detected_landmarks is None:\n",
        "            continue  # Skip frames without detected faces\n",
        "\n",
        "        landmarks = detected_landmarks[0]  # Extract first face's landmarks\n",
        "\n",
        "        # Crop ROIs and normalize the frame\n",
        "        rois = extract_roi(frame, landmarks)\n",
        "        normalized_frame = cv2.normalize(frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "\n",
        "        # Append processed frame to video frames list\n",
        "        video_frames.append({'aligned_frame': normalized_frame, 'rois': rois})\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Handle videos with insufficient frames\n",
        "    if len(video_frames) < 3:\n",
        "        excluded_videos.append(video_name)\n",
        "        continue\n",
        "    else:\n",
        "        video_frames = resample_frames(video_frames)\n",
        "\n",
        "    # Save processed frames and update stats\n",
        "    processed_frames[video_name] = video_frames\n",
        "    video_stats[\"frames_per_video\"].append(len(video_frames))\n",
        "    video_stats[\"total_frames\"] += len(video_frames)\n"
      ],
      "metadata": {
        "id": "ABUGQUhI-W24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Save Processed Data"
      ],
      "metadata": {
        "id": "7nmQELPq-c-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save processed frames to a file\n",
        "np.save('/content/drive/MyDrive/Processed_Infrared_Frames_FAN.npy', processed_frames)\n",
        "\n",
        "# Save excluded videos to a text file\n",
        "with open('/content/drive/MyDrive/Excluded_Videos.txt', 'w') as f:\n",
        "    for video in excluded_videos:\n",
        "        f.write(f\"{video}\\n\")\n",
        "\n",
        "# Print processing summary\n",
        "print(\"Processing complete. Saved processed frames.\")\n",
        "print(f\"Total Videos: {video_stats['total_videos']}\")\n",
        "print(f\"Total Frames Processed: {video_stats['total_frames']}\")\n",
        "print(f\"Average Frames Per Video: {np.mean(video_stats['frames_per_video']):.2f}\")\n",
        "print(f\"Excluded Videos: {len(excluded_videos)}\")\n"
      ],
      "metadata": {
        "id": "2r-v_bNL-eYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "49rEc8gd-hTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Post-Processing (Filtering and Statistics)"
      ],
      "metadata": {
        "id": "udwfk7cb-lsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load processed frames\n",
        "processed_frames_path = '/content/drive/MyDrive/Processed_Infrared_Frames_FAN.npy'\n",
        "processed_frames = np.load(processed_frames_path, allow_pickle=True).item()\n",
        "\n",
        "# Filter videos with fewer than 25 frames\n",
        "filtered_frames = {video_id: frames for video_id, frames in processed_frames.items() if len(frames) >= 25}\n",
        "\n",
        "# Save filtered frames\n",
        "filtered_frames_path = '/content/drive/MyDrive/Filtered_Infrared_Frames_FAN.npy'\n",
        "np.save(filtered_frames_path, filtered_frames)\n",
        "\n",
        "# Display statistics\n",
        "removed_videos = {video_id: len(frames) for video_id, frames in processed_frames.items() if len(frames) < 25}\n",
        "print(f\"Removed {len(removed_videos)} videos with fewer than 25 frames:\")\n",
        "for video_id, frame_count in removed_videos.items():\n",
        "    print(f\"Video ID: {video_id}, Frame Count: {frame_count}\")\n",
        "\n",
        "print(f\"Total videos after filtering: {len(filtered_frames)}\")"
      ],
      "metadata": {
        "id": "oPwpazcf-nnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}