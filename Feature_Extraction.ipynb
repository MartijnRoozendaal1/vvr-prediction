{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVnX2YSLaBNgsJkyFgpbZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartijnRoozendaal1/vvr-prediction/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "btKvmQJ3GEzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "UXr85gY0Fg1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Processed Frames"
      ],
      "metadata": {
        "id": "WidooTvPGKoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to preprocessed frames\n",
        "processed_frames_path = '/content/drive/MyDrive/Filtered_Infrared_Frames_FAN.npy'\n",
        "\n",
        "# Load the processed frames\n",
        "processed_frames = np.load(processed_frames_path, allow_pickle=True).item()\n",
        "\n"
      ],
      "metadata": {
        "id": "isGR4BlbFtjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-152 Feature Extraction"
      ],
      "metadata": {
        "id": "6_DEjDd9Fyxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNet-152 and remove the classification layer\n",
        "resnet152 = models.resnet152(pretrained=True)\n",
        "resnet152 = torch.nn.Sequential(*list(resnet152.children())[:-1])  # Exclude final layer\n",
        "resnet152.eval()\n",
        "\n",
        "# Transformation for ResNet-152\n",
        "transform_resnet = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract features\n",
        "resnet_features = {}\n",
        "for video_id, frames in tqdm(processed_frames.items(), desc=\"Extracting ResNet Features\"):\n",
        "    frame_features = []\n",
        "    for frame_data in frames:\n",
        "        aligned_frame = frame_data['aligned_frame']\n",
        "        image = Image.fromarray(aligned_frame).convert('RGB')\n",
        "        input_tensor = transform_resnet(image).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feature = resnet152(input_tensor).squeeze().numpy()\n",
        "            frame_features.append(feature)\n",
        "    resnet_features[video_id] = np.array(frame_features)\n",
        "\n",
        "# Save features\n",
        "resnet_features_path = '/content/drive/MyDrive/ResNet152_Features.npy'\n",
        "np.save(resnet_features_path, resnet_features)\n",
        "print(f\"ResNet-152 features saved to: {resnet_features_path}\")\n"
      ],
      "metadata": {
        "id": "jfoCIIQKGO7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG-16 Feature Extraction"
      ],
      "metadata": {
        "id": "qi0u9QaeGWbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG-16 and add Global Average Pooling (GAP)\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.eval()\n",
        "vgg16_gap = torch.nn.Sequential(\n",
        "    *list(vgg16.features),\n",
        "    torch.nn.AdaptiveAvgPool2d((1, 1))  # Reduce features to (512,)\n",
        ")\n",
        "\n",
        "# Transformation for VGG-16\n",
        "transform_vgg = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract features\n",
        "vgg_features = {}\n",
        "for video_id, frames in tqdm(processed_frames.items(), desc=\"Extracting VGG Features\"):\n",
        "    frame_features = []\n",
        "    for frame_data in frames:\n",
        "        aligned_frame = frame_data['aligned_frame']\n",
        "        image = Image.fromarray(aligned_frame).convert('RGB')\n",
        "        input_tensor = transform_vgg(image).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feature = vgg16_gap(input_tensor).squeeze().numpy()\n",
        "            frame_features.append(feature)\n",
        "    vgg_features[video_id] = np.array(frame_features)\n",
        "\n",
        "# Save features\n",
        "vgg_features_path = '/content/drive/MyDrive/VGG16_Features_512.npy'\n",
        "np.save(vgg_features_path, vgg_features)\n",
        "print(f\"VGG-16 features saved to: {vgg_features_path}\")\n"
      ],
      "metadata": {
        "id": "hUxp4XxuGaF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Feature Extraction"
      ],
      "metadata": {
        "id": "v618E1zKGb1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved features for verification\n",
        "extracted_resnet_features = np.load(resnet_features_path, allow_pickle=True).item()\n",
        "extracted_vgg_features = np.load(vgg_features_path, allow_pickle=True).item()\n",
        "\n",
        "# Check for missing features\n",
        "missing_resnet = set(processed_frames.keys()) - set(extracted_resnet_features.keys())\n",
        "missing_vgg = set(processed_frames.keys()) - set(extracted_vgg_features.keys())\n",
        "\n",
        "print(f\"Videos missing ResNet-152 features: {len(missing_resnet)}\")\n",
        "print(f\"Videos missing VGG-16 features: {len(missing_vgg)}\")\n"
      ],
      "metadata": {
        "id": "NW4Gk-emGeDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}